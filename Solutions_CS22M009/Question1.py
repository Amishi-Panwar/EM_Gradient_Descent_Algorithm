# -*- coding: utf-8 -*-
"""Que1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vlb8IgmNnKHjsdyNYy4MUfA8DgF8UnWh

**Assignment 2**







---
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import linalg as LA
import sys
import math
from matplotlib.ticker import FormatStrFormatter
from numpy.linalg import eig

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive/My\ Drive

df = pd.read_csv("Assignment PRML/A2Q1.csv",header=None)
df=np.array(df)
#no of data points given
data_points = df.shape[0]
#no of features for each data point
features = df.shape[1]

#lloyds algo for the initialization
def lloyds(df):
  for _ in range(5):
    z=[]
    K=4
    #randomly initialize assignment array
    for i in range(data_points):
      rand_point=np.random.randint(0,4)
      z.append(rand_point)

    clusters=[]
    for i in range(K):
        clusters.append([])
    for i in range(data_points):
        clusters[z[i]].append(i)

    #calculate mean of clusters
    centroids=np.zeros((K,features))
    error=[]
    iteration=[]
    for itr in range(500):
        flag=False
        for i in range(len(clusters)):
          sum=np.sum(df[clusters[i]],axis=0)
          if(len(clusters[i])!=0):
              new_mean=sum/len(clusters[i])
              centroids[i]=new_mean
        centroids=centroids[~np.all(centroids == 0, axis=1)]
    

        error_sum=0
        newclusters=[]
        for i in range(K):
            newclusters.append([])
        newz=[]
        #Assign datapoints to nearest mean
        for i in range(data_points):
            min_dist=sys.maxsize
            index=0
            for j in range(K):
                dist=np.sum((df[i]-centroids[j])**2)
                if(min_dist>dist):
                    index=j
                    min_dist=dist
            if(z[i]!=index):
              flag=True
            newclusters[index].append(i)
            newz.append(index)
            error_sum=error_sum+min_dist
        error.append(error_sum)
        iteration.append(itr)
        if(flag==False):
          break
        clusters=newclusters
        z=newz

    centroids = np.array(centroids)
    #return mean of clusters as centroids, assignment array(z), 
    return centroids, z, iteration, error

"""**Quetion.1 Part-(i)**"""

def compute_lambda(count1, pi, probability):
  lambdas = []
  for i in range(0,400):
    no_of_1_in_row = count1[i]
    lamb = []
    for j in range(0,4):
      numerator = pi[j]*pow(probability[j],no_of_1_in_row)*pow((1 - probability[j]),50 - no_of_1_in_row)
      denominator = 0
      for k in range(0,4):
        denominator = denominator + pi[k]*pow((1 - probability[k]),50 - no_of_1_in_row)*pow(probability[k],no_of_1_in_row)
      lamb.append(numerator/denominator)
    lambdas.append(lamb)
  return lambdas

def compute_probability(lambdas, count1):
  probability = []
  for k in range(0,4):
    numerator = 0
    denominator = 0
    for i in range(0,400):
      numerator += lambdas[i][k]* (count1[i]/50)
      denominator += lambdas[i][k]
    probability.append(numerator/denominator)
  return probability

def compute_pi(lambdas):
  pi = []
  for k in range(0,4):
    numerator = 0
    for i in range(0,400):
      numerator += lambdas[i][k]
    pi.append(numerator/400)
  return pi

def compute_maxLikelihood(probability, pi, count1):
  outersum = 0
  for i in range(0,400):
    innersum = 0
    for k in range(0,4):
      innersum += pi[k]*pow(probability[k],count1[i])*pow(1 - probability[k],50 - count1[i])
    outersum += math.log(innersum)
  return outersum

#EM Algorithm
itr = []
maximumLikelihoodAll= []
for l in range(0,100):
  centroids, z, iteration, error = lloyds(df)
  probability = centroids.mean(axis=1)

  #calculating pi for each cluster formed by lloyds algo 
  pi = np.zeros(4)
  for i in range(0,400):
    if(z[i] == 0):
      pi[0] += 1
    elif(z[i] == 1):
      pi[1] += 1
    elif(z[i] == 2):
      pi[2] += 1
    elif(z[i] == 3):
      pi[3] += 1
  pi = pi/data_points

  #count no of 1's in each data point
  count1= []
  for i in range(0, 400):
    sum = np.sum(df[i,:])
    count1.append(sum)

  #store prev data, and calculate new data  
  prev_prob = probability
  prev_pi = pi
  maxLogLikelihood = []
  iterations = []
  iteration = 0

  while True:
    iteration += 1
    iterations.append(iteration)

    #compute lambda
    lambdas = []
    lambdas = compute_lambda(count1, pi, probability)

    #compute probability
    probability = []
    probability = compute_probability(lambdas, count1)

    #compute pi
    pi = []
    pi = compute_pi(lambdas)
    
    #compute maximum_likelihood
    sum = compute_maxLikelihood(probability, pi, count1)
    maxLogLikelihood.append(sum)
    
    #convert lists into numpy array
    probability = np.array(probability)
    prev_prob = np.array(prev_prob)

    #calculate error b/w prev and current probabilties
    prob_diff = probability - prev_prob
    prob_error = np.sqrt(np.dot(prob_diff.T, prob_diff))

    #convert lists into numpy array
    pi = np.array(pi)
    prev_pi = np.array(prev_pi)

    #calculate error b/w prev and current pi
    pi_diff = pi - prev_pi
    pi_error = np.sqrt(np.dot(pi_diff.T, pi_diff))
    
    tolerance = 1e-7
    #if error is less than tolerance then algo converged else repeat procedure with new values 
    if(prob_error <= tolerance and pi_error <= tolerance):
      break
    else:
      prev_prob = probability
      prev_pi = pi
      
  itr.append(iteration)
  maximumLikelihoodAll.append(maxLogLikelihood)

# find max no of iterations 
max = 0
for i in range(0,len(itr)):
  if itr[i] > max:
    max = itr[i]

# make all the maximumLikelihood lists of same size as of max itr
for i in range(0,len(maximumLikelihoodAll)):
  last_index = len(maximumLikelihoodAll[i])
  if(last_index < max):
    to_append = max - last_index
    while(to_append > 0):
      maximumLikelihoodAll[i].append(maximumLikelihoodAll[i][last_index-1])
      to_append =to_append-1

#average of maxlikelihoods in each iteration
average_likelihood = np.average(np.array(maximumLikelihoodAll),axis = 0)
itr = []
for i in range(max):
  itr.append(i)

#plot iterations v/s log likelihood
plt.title('Iterations v/s Log Likelihood')
plt.xlabel('Iterations')
plt.ylabel('Log Likelihood')
plt.plot(itr,average_likelihood)
plt.show()

"""**Quetion.1 Part-(ii)**"""

def compute_lambdas(df, mean, sigmas, pi):
  lambdas = []
  for i in range(0,400):
    lamb = []
    for j in range(0,4):
      value1 = np.exp(-0.5*np.matmul(np.matmul((df[i]- mean[j]),np.linalg.pinv(sigmas[j])),(np.array(df[i]- mean[j]).T)))
      eigenval, eigenvec = eig(np.array(sigmas[j]))
      nonZeroEigenValues = []
      #calculate pseudo determinant
      for e in eigenval:
        if(e > 1e-5):
          nonZeroEigenValues.append(e)
      pseudoDet = np.product(nonZeroEigenValues)
      value2 = np.sqrt(pow(2*np.pi , 50)* pseudoDet)

      numerator = (value1/value2)*pi[j]

      denominator = 0
      for k in range(0,4):
        value1 = np.exp(-0.5*np.matmul(np.matmul((df[i]- mean[k]),np.linalg.pinv(sigmas[k])),(np.array(df[i]- mean[k]).T)))          
        eigenval, eigenvec = eig(sigmas[k])
        nonZeroEigenValues = []
        for e in eigenval:
          if(e > 1e-5):
            nonZeroEigenValues.append(e)
        pseudoDet = np.product(nonZeroEigenValues)
        value2 = np.sqrt(pow(2*np.pi , 50)* pseudoDet)
        sum = (value1/value2)*pi[k]
        denominator = denominator + sum
      lamb.append(numerator/denominator)
    lambdas.append(lamb)
  return lambdas

def compute_mean(lambdas, df):
  u = []
  for k in range(0,4):
    numerator = 0
    denominator = 0
    for i in range(0,400):
      numerator += lambdas[i][k]*(df[i])
      denominator += lambdas[i][k]
    u.append(numerator/denominator)
  return u

def compute_pi(lambdas):
  pi = []
  for k in range(0,4):
    numerator = 0
    for i in range(0,400):
      numerator += lambdas[i][k]
    pi.append(numerator/400)
  return pi

def compute_sigmas(lambdas):
  sigmas = []
  for k in range(0,4):
    numerator = [[0]*50]*50
    denominator = 0

    for i in range(0,400):
      d_u = np.array(df[i]- u[k])
      d_u_t = np.zeros((1,50))
    
      for i in range(0,50):
        d_u_t[0][i] = d_u[i] 
      
      numerator = np.add(numerator, lambdas[i][k]*np.matmul(d_u_t,d_u))
      denominator += lambdas[i][k]

    sigmas.append(numerator/denominator)
  return sigmas

def compute_maximumLikelihood(mean, sigmas):
  max = 0
  for i in range(0,400):
    denominator = 0
    for k in range(0,4):  
      value1 = np.exp(-0.5*np.matmul(np.matmul((df[i]- mean[k]),np.linalg.pinv(sigmas[k])),(np.array(df[i]- mean[k]).T)))        
      eigenval, eigenvec = eig(sigmas[k])
      nonZeroEigenValues = []
      for e in eigenval:
        if(e > 1e-5):
          nonZeroEigenValues.append(e)
      pseudoDet = np.product(nonZeroEigenValues)
      value2 = np.sqrt(pow(2*np.pi , 50)* pseudoDet)
      sum = (value1/value2)*pi[k]
      denominator = denominator + sum
    max+= math.log(denominator)
  return max

itr = []
maximumLikelihoodAll= []
for l in range(0,2):
  mean, z, iteration, error = lloyds(df)

  #pi
  pi = np.zeros(4)
  for i in range(0,400):
    if(z[i] == 0):
      pi[0] += 1
    elif(z[i] == 1):
      pi[1] += 1
    elif(z[i] == 2):
      pi[2] += 1
    elif(z[i] == 3):
      pi[3] += 1
  pi = pi/data_points

  #computing sigma 
  sigmas = []
  t = [[[0]*50]*50]*4
  count = np.zeros(4)
  for i in range(0,400):
    if(z[i] == 0):
      count[0] += 1
      t[0] = np.add(t[0],np.matmul(np.array(df[i]).T-np.array(mean[0]).T,np.array(df[i]-mean[0])))
    elif(z[i] == 1):
      count[1] += 1
      t[1] = np.add(t[1],np.matmul(np.array(df[i]).T-np.array(mean[1]).T,np.array(df[i]-mean[1])))
    elif(z[i] == 2):
      count[2] += 1
      t[2] = np.add(t[2],np.matmul(np.array(df[i]).T-np.array(mean[2]).T,np.array(df[i]-mean[2])))
    elif(z[i] == 3):
      count[3] += 1
      t[3] = np.add(t[3],np.matmul(np.array(df[i]).T-np.array(mean[3]).T,np.array(df[i]-mean[3])))

  for i in range(0,4):
    sigmas.append(t[i]/count[i])
  
  #storing prev values
  prev_mean = mean
  prev_pi = pi
  prev_sigma = sigmas
  maxLogLikelihood = []
  iterations = []
  iteration = 0

  while True:
    iteration += 1
    iterations.append(iteration)

    #computelambda
    lambdas = []
    lambdas = compute_lambdas(df, mean, sigmas, pi)
    
    #computing mean
    mean = []
    mean = compute_mean(lambdas, df)    

    #compute pi
    pi = []
    pi = compute_pi(lambdas)

    #compute sigma
    sigmas = []
    sigmas = compute_sigmas(lambdas)
    
    sum = compute_maximumLikelihood(mean, sigmas)
    maxLogLikelihood.append(sum)

    #convert list to array
    mean = np.array(mean)
    prev_mean = np.array(prev_mean)

    diff_mean = []
    for i in range(0,4):
      d = mean[i] - prev_mean[i]
      diff_mean.append(d)
    
    sum = np.zeros(50)
    for i in range(0,len(diff_mean)):
      sum = np.add(sum,diff_mean[i])

    d = sum
    diff_mean = (np.sqrt(np.dot(np.array(d.T),d)))

    #convert list to array
    pi = np.array(pi)
    prev_pi = np.array(prev_pi)

    diff = pi - prev_pi
    diff_pi = (np.sqrt(np.dot(np.array(diff.T),diff)))


    diff = np.array(sigmas).flatten() - np.array(prev_sigma).flatten()
    diff_sigma = (np.sqrt(np.dot(np.array(diff).T,diff)))   
    diff_sigma = diff_sigma/2500 
    tolerance = 1e-7
    if(diff_mean <= tolerance and diff_pi <= tolerance and diff_sigma <= tolerance):
      break
    else:
      prev_mean = mean
      prev_pi = pi
      prev_sigma = sigmas

  itr.append(iteration)
  maximumLikelihoodAll.append(maxLogLikelihood)

#find max iterations
max = 0
for i in range(0,len(itr)):
  if itr[i] > max:
    max = itr[i]

print(len(itr))
print(len(maximumLikelihoodAll[0]))

#append converged value at last to make size of all lists equal
for i in range(0,len(maximumLikelihoodAll)):
  last_index = len(maximumLikelihoodAll[i])
  if(last_index < max):
    to_append = max - last_index
    while(to_append > 0):
      maximumLikelihoodAll[i].append(maximumLikelihoodAll[i][last_index-1])
      to_append = to_append-1

#taking average over all iterations
average_likelihood = np.average(np.array(maximumLikelihoodAll),axis = 0)
itr = []
for i in range(0,max):
  itr.append(i)
  
print(len(itr))
print(average_likelihood.shape)

# plotting iteration v/s log_likelihood
plt.title('Iteration v/s log_likelihood')
plt.xlabel('Iteration')
plt.ylabel('Log likelihood')
plt.plot(itr,average_likelihood)
plt.show()

"""**Question.1 Part(iii)**"""

#Applying k-means clustering
mean,z,iteration,error_vector = lloyds(df)

#plotting Iterations v/s Error
plt.title('Iteration v/s Error')
plt.xlabel('Iteration')
plt.ylabel('Error')
plt.plot(iteration,error_vector)
plt.show()
